{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca9fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Abrimos fichero\n",
    "# Introducir en path_file la ruta donde hemos descargado el fichero\n",
    "\n",
    "path_file = \"XXXXX\"\n",
    "df = pd.read_csv(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "221431fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos en train y test\n",
    "df_train, df_test = train_test_split(df, test_size=0.25, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "610e0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos variables que no vamos a usar: \n",
    "    \n",
    "    # \"trans_date_trans_time\", \"date\" y \"time\" por ser redundantes con el resto de variables date/time\n",
    "    # \"cc_num\" por no aportar información relevante\n",
    "    # \"city\", \"state\" y \"zip\" por ser redundantes con las variables 'lat' y 'long'\n",
    "\n",
    "vars_todrop = [\"trans_date_trans_time\", \"cc_num\", \"city\", \"state\", \"zip\", \"date\", \"time\"]\n",
    "df_train = df_train.drop(vars_todrop, axis=1)\n",
    "\n",
    "# Creamos las variables \"dist_lat\" y \"dist_long\" como medidas de distancia del lugar de la transacción respecto al domicilio del titular de la tarjeta\n",
    "\n",
    "df_train[\"dist_lat\"] = df_train[\"lat\"] - df_train[\"merch_lat\"]\n",
    "df_train[\"dist_long\"] = df_train[\"long\"] - df_train[\"merch_long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf4d20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos NA's y NULL's\n",
    "\n",
    "df_train.isna().sum()\n",
    "\n",
    "# Procesamos los NA's de las variables \"state_abbr\" y \"Unemployed percent\":\n",
    "    # En el caso de \"state_abbr\" con la categoría \"NoNamed\"\n",
    "    # En el caso de \"Unemployed percent\" con su valor promedio\n",
    "\n",
    "df_train['state_abbr'].fillna(\"NoNamed\", inplace = True)\n",
    "df_train['Unemployed percent'].fillna(df_train['Unemployed percent'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aed5b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos las variables categóricas en one-hot\n",
    "\n",
    "categorical_vars = [col for col in df_train.columns if df_train[col].dtype == 'object']\n",
    "df_train = pd.get_dummies(df_train, columns=categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "447f80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos una categoría \"_Others\" a cada una de las variables categóricas. Como veremos más abajo servirá para garantizar presencia de las mismas variables en train y test, asimilando ambos conjuntos\n",
    "\n",
    "for var in categorical_vars:\n",
    "    df_train[var+\"_Others\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b354f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Instalamos e importamos librerías\n",
    "\n",
    "!pip install imbalanced-learn\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Corregimos el desbalanceo de la variable objetivo \"is_fraud\" haciendo undersampling, buscamos un equilibrio: buen aprendizaje de la categoría minoritaria ('is_fraud' = 1) sin descuidar el aprendizaje de la categoría mayoritaria ('is_fraud' = 0)\n",
    "\n",
    "X = df_train.drop('is_fraud', axis = 1)\n",
    "y = df_train['is_fraud']\n",
    "\n",
    "resampler = RandomUnderSampler(sampling_strategy ='auto', random_state=17)\n",
    "X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.DataFrame(X_resampled, columns = X.columns)\n",
    "df_train['is_fraud'] = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44479047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hacemos todas las transformaciones de train en el fichero de test\n",
    "\n",
    "# Eliminamos variables que no vamos a usar\n",
    "\n",
    "df_test = df_test.drop(vars_todrop, axis=1)\n",
    "\n",
    "# Creamos las variables \"dist_lat\" y \"dist_long\"\n",
    "\n",
    "df_test[\"dist_lat\"] = df_test[\"lat\"] - df_test[\"merch_lat\"]\n",
    "df_test[\"dist_long\"] = df_test[\"long\"] - df_test[\"merch_long\"]\n",
    "\n",
    "# Procesamos los NA's de las variables \"state_abbr\" y \"Unemployed percent\"\n",
    "\n",
    "df_test['state_abbr'].fillna(\"NoNamed\", inplace = True)\n",
    "df_test['Unemployed percent'].fillna(df_test['Unemployed percent'].mean(), inplace = True)\n",
    "\n",
    "# Transformamos las variables categóricas en one-hot\n",
    "\n",
    "df_test = pd.get_dummies(df_test, columns=categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faf0c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apoyándonos en las categorías \"var+_Others\" (añadidas más arriba) creamos, eliminamos y transformamos variables para asimilar los conjuntos de train y test\n",
    "\n",
    "vars_no_incommon_train = [var for var in df_train.columns if var not in df_test.columns]\n",
    "vars_no_incommon_test = [var for var in df_test.columns if var not in df_train.columns]\n",
    "\n",
    "for var in vars_no_incommon_train:\n",
    "    df_test[var] = 0\n",
    "for var in vars_no_incommon_test:\n",
    "    df_test = df_test.drop(var, axis = 1)\n",
    "    df_test[var+\"_Others\"] = 1\n",
    "\n",
    "df_train_columnssorted = df_train.columns\n",
    "df_train = df_train[df_train_columnssorted]\n",
    "df_test = df_test[df_train_columnssorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7452f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos los datos\n",
    "\n",
    "# Dividimos entre X (variables predictoras) e y (variable a predecir)\n",
    "\n",
    "X_train = df_train.drop('is_fraud', axis = 1)\n",
    "y_train = df_train['is_fraud']\n",
    "\n",
    "# Subdividimos X_train entre variables a normalizar y las que no normalizaremos\n",
    "\n",
    "variables_tonorm = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'Year', 'Month', 'Unemployed percent', 'change_usd_eur', 'age', 'week_number', 'day', 'hour', 'crime_rate', 'dist_lat', 'dist_long']\n",
    "X_train_tonorm = X_train[variables_tonorm]\n",
    "X_train_notnorm = X_train.drop(variables_tonorm, axis = 1)\n",
    "\n",
    "# Después normalizamos\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler().fit(X_train_tonorm)\n",
    "X_train_normalized = scaler_X.transform(X_train_tonorm)\n",
    "\n",
    "# Por último volvemos a unir el X_train con las variables normalizadas y las no-normalizadas\n",
    "\n",
    "X_train_notnorm = X_train_notnorm.values\n",
    "X_train = np.concatenate([X_train_normalized, X_train_notnorm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ac7b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos lo mismo en test, con la normalización (scaler) de train\n",
    "\n",
    "X_test = df_test.drop('is_fraud', axis = 1)\n",
    "y_test = df_test['is_fraud']\n",
    "\n",
    "X_test_tonorm = X_test[variables_tonorm]\n",
    "X_test_notnorm = X_test.drop(variables_tonorm, axis = 1)\n",
    "\n",
    "X_test_normalized = scaler_X.transform(X_test_tonorm)\n",
    "\n",
    "X_test_notnorm = X_test_notnorm.values\n",
    "X_test = np.concatenate([X_test_normalized, X_test_notnorm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fcef9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo clf: 0.9641588515630567\n",
      "Precisión del modelo clf: 0.12376211407085738\n",
      "Recall del modelo clf: 0.9782335705316032\n",
      "Verdaderos Positivos: 2337\n",
      "Falsos Negativos: 52\n",
      "Falsos Positivos: 16546\n",
      "Verdaderos Negativos: 444164\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score,roc_auc_score, roc_curve, f1_score\n",
    "\n",
    "# Modelo 1: árbol de decisión sencillo\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=17)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[1, 1]\n",
    "FN = cm[1, 0]\n",
    "FP = cm[0, 1]\n",
    "TN = cm[0, 0]\n",
    "print(\"Accuracy del modelo clf:\", accuracy)\n",
    "print(\"Precisión del modelo clf:\", precision)\n",
    "print(\"Recall del modelo clf:\", recall)\n",
    "print(\"Verdaderos Positivos:\", TP)\n",
    "print(\"Falsos Negativos:\", FN)\n",
    "print(\"Falsos Positivos:\", FP)\n",
    "print(\"Verdaderos Negativos:\", TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34a465bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo clf: 0.9794709122671394\n",
      "Precisión del modelo clf: 0.19622738135882553\n",
      "Recall del modelo clf: 0.9623273336123901\n",
      "Verdaderos Positivos: 2299\n",
      "Falsos Negativos: 90\n",
      "Falsos Positivos: 9417\n",
      "Verdaderos Negativos: 451293\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modelo 2: random forest\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 200, random_state = 17)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[1, 1]\n",
    "FN = cm[1, 0]\n",
    "FP = cm[0, 1]\n",
    "TN = cm[0, 0]\n",
    "print(\"Accuracy del modelo clf:\", accuracy)\n",
    "print(\"Precisión del modelo clf:\", precision)\n",
    "print(\"Recall del modelo clf:\", recall)\n",
    "print(\"Verdaderos Positivos:\", TP)\n",
    "print(\"Falsos Negativos:\", FN)\n",
    "print(\"Falsos Positivos:\", FP)\n",
    "print(\"Verdaderos Negativos:\", TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e0fc1",
   "metadata": {},
   "source": [
    "### A modo de conclusión\n",
    "\n",
    "Los modelos buscan encontrar cierto equilibrio entre recall (como métrica base) y accuracy, capaces de detectar la práctica totalidad del fraude sin que se disparen los falsos positivos que marcarían como fraudulentas transacciones que no lo son.\n",
    "\n",
    "En este sentido ambos modelos consiguen alcanzar ese equilibrio con métricas más que aceptables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
